<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Face Detection with Real-Time Bounding Box</title>
  <style>
    #video-container {
      position: relative;
      display: inline-block;
    }
    #video {
      display: block;
      max-width: 640px;
      height: auto;
    }
    #overlay {
      position: absolute;
      top: 0;
      left: 0;
      z-index: 1;
      pointer-events: none;
    }
    #photo {
      display: block;
      margin-top: 10px;
      max-width: 100%;
    }
  </style>
</head>
<body>
  <h1>Face Detection & Capture</h1>
  <div id="video-container">
    <video id="video" autoplay playsinline></video>
    <canvas id="overlay"></canvas>
  </div>
  <button id="capture">Capture Face</button>
  <img id="photo" alt="Captured Face" />

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>

  <script>
    document.addEventListener("DOMContentLoaded", async () => {
      const video = document.getElementById('video');
      const overlay = document.getElementById('overlay');
      const photo = document.getElementById('photo');
      const captureButton = document.getElementById('capture');

      const ctx = overlay.getContext('2d');

      // Access the camera
      navigator.mediaDevices.getUserMedia({ video: { facingMode: "user" } })
        .then(stream => {
          video.srcObject = stream;

          video.addEventListener('loadeddata', () => {
            overlay.width = video.videoWidth;
            overlay.height = video.videoHeight;
            setupFaceDetection();
          });
        })
        .catch(err => console.error("Error accessing camera: ", err));

      // Load BlazeFace model and set up detection
      let model;
      async function setupFaceDetection() {
        model = await blazeface.load();
        console.log("BlazeFace model loaded.");
        detectFace();
      }

      async function detectFace() {
        if (!model) return;

        const predictions = await model.estimateFaces(video, false);

        // Clear the previous frame
        ctx.clearRect(0, 0, overlay.width, overlay.height);

        if (predictions.length > 0) {
          predictions.forEach(prediction => {
            const start = prediction.topLeft;
            const end = prediction.bottomRight;
            const size = [end[0] - start[0], end[1] - start[1]];

            // Draw the bounding box
            ctx.strokeStyle = "red";
            ctx.lineWidth = 2;
            ctx.strokeRect(start[0], start[1], size[0], size[1]);
          });
        }

        requestAnimationFrame(detectFace);
      }

      // Capture the face inside the bounding box
      captureButton.addEventListener('click', () => {
        if (!model) return;

        model.estimateFaces(video, false).then(predictions => {
          if (predictions.length > 0) {
            const prediction = predictions[0]; // Use the first detected face
            const start = prediction.topLeft;
            const end = prediction.bottomRight;

            const width = end[0] - start[0];
            const height = end[1] - start[1];

            // Create a temporary canvas to extract the face
            const tempCanvas = document.createElement('canvas');
            const tempCtx = tempCanvas.getContext('2d');
            tempCanvas.width = width;
            tempCanvas.height = height;

            tempCtx.drawImage(
              video,
              start[0], start[1], width, height, // Source coordinates
              0, 0, width, height               // Destination coordinates
            );

            // Display the face
            photo.src = tempCanvas.toDataURL('image/png');
          } else {
            alert("No face detected. Try again!");
          }
        });
      });
    });
  </script>
</body>
</html>
